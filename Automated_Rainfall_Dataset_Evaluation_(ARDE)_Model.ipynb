{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDW01rhg-GK5"
      },
      "source": [
        "I propose the **Automated Rainfall Dataset Evaluation (ARDE)** Model, a Python-based workflow for comparing station-level rainfall gauge data with multiple satellite and reanalysis products."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3AEV2qR-21W"
      },
      "source": [
        "**Multiple rainfall data sources will be analyzed, including:**\n",
        "\n",
        "\n",
        "*   IMD Gridded data\n",
        "*   ERA5 Reanalysis - [link text](https://cds.climate.copernicus.eu/)\n",
        "*   CHIRPS -[link text](https://www.chc.ucsb.edu/data/chirps)\n",
        "*   GPM - IMERG - [link text](https://disc.gsfc.nasa.gov/datasets/GPM_3IMERGDF_07/summary?keywords=imerg%20daily)\n",
        "\n",
        "**Other datasets which are not included in this study:**\n",
        "\n",
        "\n",
        "*   GSMap ISRO Rain data\n",
        "*   CHRS Persiann\n",
        "*   Indian Monsoon Data Assimilation and Analysis (IMDAA)- [link text](https://rds.ncmrwf.gov.in/datasets)\n",
        "\n",
        "The performance of these datasets will be validated against ground-based rain gauge observations obtained from **NWIC-IndiaWRIS** at selected sites within the basin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTCMaFvLLQ62"
      },
      "outputs": [],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnzIP7uPEY1U"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "# Read YAML config\n",
        "with open(\"config.yaml\", \"r\") as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "# Extract paths and parameters\n",
        "data_dir = cfg[\"data_dir\"]\n",
        "output_dir = cfg[\"output_dir\"]\n",
        "lat = cfg.get(\"lat\")\n",
        "lon = cfg.get(\"lon\")\n",
        "station_name = cfg.get(\"station_name\", \"Unknown Station\")\n",
        "start_date = cfg.get(\"start_date\")\n",
        "end_date = cfg.get(\"end_date\")\n",
        "cds_url = cfg.get(\"cds_url\")\n",
        "cds_key = cfg.get(\"cds_key\")\n",
        "gee_project_id = cfg.get(\"gee_project_id\")\n",
        "\n",
        "# Create folders if they don't exist\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Print configuration\n",
        "print(\"Data directory:\", data_dir)\n",
        "print(\"Output directory:\", output_dir)\n",
        "print(f\"Station: {station_name} ({lat}, {lon})\")\n",
        "print(f\"Start date: {start_date}, End date: {end_date}\")\n",
        "print(f\"CDS API URL: {cds_url}\")\n",
        "print(f\"CDS API Key: {cds_key}\")\n",
        "print(f\"GEE Project ID: {gee_project_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp8aC55e_5pC"
      },
      "source": [
        "# **IMD Gridded Rainfall Data (IMD4)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dASQIG5tkIt"
      },
      "source": [
        "| Attribute              | Details                                                                 |\n",
        "|------------------------|-------------------------------------------------------------------------|\n",
        "| **Product Name**       | IMD Gridded Rainfall Data (IMD 4)                                       |\n",
        "| **Spatial Resolution** | 0.25° × 0.25° (~25 km × 25 km)                                          |\n",
        "| **Spatial Coverage**   | Indian subcontinent; arranged as a 135 × 129 grid from ~6.5° N, 66.5° E to ~38.5° N, 100° E |\n",
        "| **Temporal Coverage**  | 1901 – 2022 (ongoing updates available)|\n",
        "| **Temporal Resolution**| Daily rainfall values in millimeters (mm) |\n",
        "| **Number of Stations** | Based on data from ~6,955 rain gauge stations (highest in country) |\n",
        "| **Data Latency**       | Typically ~3 days |\n",
        "| **Data Format**        | NetCDF (.nc), with annual files per year (e.g., `RF25_indYYYY_rfp25.nc`) |\n",
        "| **Key Features**       | Long-term, high-resolution daily rainfall reference for India; widely used for monsoon, agricultural, hydrological studies|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdKpRJeNuGyW"
      },
      "source": [
        "**Downloading and Extracting IMD Gridded Rainfall Data for a Specific Location**\n",
        "\n",
        "This Python script uses the imdlib library to download daily IMD gridded rainfall data for specified time range, extracts the rainfall time series for the nearest grid point and saves it as a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEQRDen3AH7K"
      },
      "outputs": [],
      "source": [
        "import imdlib as imd\n",
        "\n",
        "start_dy, end_dy = start_date,end_date\n",
        "var_type = 'rain'\n",
        "\n",
        "imd_data_dir = os.path.join(data_dir, \"IMD\")\n",
        "os.makedirs(imd_data_dir, exist_ok=True)\n",
        "\n",
        "df_imd = imd.get_real_data(var_type, start_dy, end_dy, imd_data_dir)\n",
        "df_imd.to_csv(f\"imdlib_{start_dy[:4]}_{end_dy[:4]}.csv\",\n",
        "            lat=lat, lon=lon, out_dir=imd_data_dir)\n",
        "print(\"csv file saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvjQeUbiAMF3"
      },
      "outputs": [],
      "source": [
        "ds=df_imd.get_xarray()\n",
        "print(len(ds.rain))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHKiUq6BAROY",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "ds = ds.where(ds['rain']!=-999.)\n",
        "ds['rain'].mean('time').plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72sl7tsnAhrS"
      },
      "source": [
        "# **ERA5 Hourly Data on Single Levels - Total Precipitation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrbpzMS_v8Y5"
      },
      "source": [
        "| Attribute              | Details                                                                 |\n",
        "|------------------------|-------------------------------------------------------------------------|\n",
        "| **Product Name**       | ERA5 Hourly Data on Single Levels – Total Precipitation                 |\n",
        "| **Coverage**| Global                                                                  |\n",
        "| **Horizontal Resolution** | Atmosphere: 0.25° × 0.25°; Ocean waves: 0.5° × 0.5°                   |\n",
        "| **Temporal Coverage**  | 1940 – present                                                          |\n",
        "| **Temporal Resolution**| Hourly                                                                  |\n",
        "| **File Format**        | GRIB or NetCDF                                                                   |\n",
        "| **Update Frequency**   | Daily                                                                   |\n",
        "| **Main Variable**      | Total precipitation (m of water equivalent)                             |\n",
        "| **Variable Definition**| Accumulated liquid + frozen water (rain + snow) reaching Earth’s surface; sum of large-scale + convective precipitation. Excludes fog, dew, and evaporation before reaching surface. |\n",
        "| **Units**              | metres of water equivalent                                              |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK9cajFWwGSM"
      },
      "source": [
        "**Configuring CDS API Credentials for Data Access**\n",
        "\n",
        "This code creates a new cdsapirc file containing the Copernicus Climate Data Store (CDS) API URL and your personal access key, enabling authenticated downloads of climate datasets via the CDS API.\n",
        "\n",
        "Note: To run this code, you must have a CDS API Access Key.\n",
        "\n",
        "**How to get your key:**\n",
        "\n",
        "\n",
        "1.   Create an account at https://cds.climate.copernicus.eu.\n",
        "2.   Log in and go to your User Profile.\n",
        "1.   Under the API key section, copy the provided url and key details.\n",
        "2.   Replace the placeholder in the `config.yaml` with your actual key.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SPXyFg0Xzp7",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "\n",
        "\n",
        "# Load YAML config\n",
        "with open(\"config.yaml\", \"r\") as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "# Get CDS API details from YAML\n",
        "cds_url = cfg.get(\"cds_url\")\n",
        "cds_key = cfg.get(\"cds_key\")\n",
        "\n",
        "if not cds_url or not cds_key:\n",
        "    raise ValueError(\"CDS API URL or Key is missing in config.yaml\")\n",
        "\n",
        "# Path to .cdsapirc in user's home directory\n",
        "cdsapirc_path = os.path.join(os.path.expanduser(\"~\"), \".cdsapirc\")\n",
        "\n",
        "# Write CDS API config\n",
        "with open(cdsapirc_path, 'w') as f:\n",
        "    f.write(f\"url: {cds_url}\\n\")\n",
        "    f.write(f\"key: {cds_key}\\n\")\n",
        "\n",
        "print(f\".cdsapirc file created at: {cdsapirc_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIRXe3FBx2xz"
      },
      "source": [
        "**Downloading ERA5 Total Precipitation Data for a Specific Region and Period**\n",
        "\n",
        "This Python script uses the cdsapi library to download ERA5 reanalysis total precipitation data (hourly, GRIB format) for the years selected years over a defined bounding box in the specified region. The downloaded dataset is saved locally for further processing and analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd6gByUxXSmk"
      },
      "outputs": [],
      "source": [
        "import cdsapi\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "# --- Load YAML config ---\n",
        "with open(\"config.yaml\", \"r\") as f:  # Adjust path if needed\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "data_dir = cfg[\"data_dir\"]\n",
        "lat = cfg[\"lat\"]\n",
        "lon = cfg[\"lon\"]\n",
        "start_dy = cfg[\"start_date\"]\n",
        "end_dy = cfg[\"end_date\"]\n",
        "\n",
        "# Create ERA5 output folder\n",
        "era5_dir = os.path.join(data_dir, \"ERA5\")\n",
        "os.makedirs(era5_dir, exist_ok=True)\n",
        "\n",
        "# Extract years from start & end date\n",
        "years = list(range(int(start_dy[:4]), int(end_dy[:4]) + 1))\n",
        "years = [str(y) for y in years]\n",
        "\n",
        "# Initialize CDS API client\n",
        "c = cdsapi.Client()\n",
        "\n",
        "# Download ERA5 data\n",
        "c.retrieve(\n",
        "    \"reanalysis-era5-single-levels\",\n",
        "    {\n",
        "        \"product_type\": \"reanalysis\",\n",
        "        \"format\": \"grib\",\n",
        "        \"variable\": \"total_precipitation\",\n",
        "        \"year\": years,\n",
        "        \"month\": [f\"{m:02d}\" for m in range(1, 13)],\n",
        "        \"day\": [f\"{d:02d}\" for d in range(1, 32)],\n",
        "        \"time\": [f\"{h:02d}:00\" for h in range(24)],\n",
        "        \"area\": [lat + 0.724, lon - 0.762, lat - 0.510, lon + 0.353],\n",
        "    },\n",
        "    os.path.join(era5_dir, f\"era5_{start_dy[:4]}_{end_dy[:4]}.grib\"),\n",
        ")\n",
        "\n",
        "print(f\"ERA5 data saved to: {era5_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O56-ZHMYyU3n"
      },
      "source": [
        "**Processing ERA5 GRIB Precipitation Data into Daily Rainfall (mm) and Saving as NetCDF**\n",
        "\n",
        "This script reads ERA5 total precipitation data from a GRIB file using xarray with the cfgrib engine, converts units from meters to millimeters, computes actual timestamps from time and step variables, aggregates hourly values into daily totals, and saves the processed data as a NetCDF file for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R9mWhPri0Xs"
      },
      "outputs": [],
      "source": [
        "import xarray as xr\n",
        "import rioxarray\n",
        "\n",
        "input_grib = os.path.join(data_dir, \"ERA5\", f\"era5_{start_dy[:4]}_{end_dy[:4]}.grib\")\n",
        "output_nc = os.path.join(data_dir, \"ERA5\", f\"era5_{start_dy[:4]}_{end_dy[:4]}.nc\")\n",
        "\n",
        "ds = xr.open_dataset(input_grib, engine=\"cfgrib\")\n",
        "tp = ds[\"tp\"] * 1000\n",
        "tp.name = \"tp\"\n",
        "tp.attrs[\"units\"] = \"mm\"\n",
        "\n",
        "valid_time = (ds.time.values[:, None] + ds.step.values).flatten()\n",
        "tp_stacked = tp.stack(z=('time', 'step')).transpose('z', 'latitude', 'longitude')\n",
        "tp_stacked.coords['datetime'] = ('z', valid_time)\n",
        "\n",
        "tp_time = tp_stacked.swap_dims({'z': 'datetime'}).sortby('datetime')\n",
        "tp_daily = tp_time.resample(datetime='1D').sum().dropna('datetime', how='any')\n",
        "\n",
        "tp_daily.to_netcdf(output_nc)\n",
        "print(f\"Saved daily rainfall in mm as NetCDF: {output_nc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOznYte8yjCE"
      },
      "source": [
        "**Extracting ERA5 Daily Rainfall for a Specific Location and Saving as CSV**\n",
        "\n",
        "This script loads processed ERA5 daily rainfall data from a NetCDF file, extracts the time series for the nearest grid point to specified Location, converts it to a pandas DataFrame, and saves it as a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVc2W-u0OKK7"
      },
      "outputs": [],
      "source": [
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load the NetCDF file\n",
        "nc_file = os.path.join(data_dir, \"ERA5\", f\"era5_{start_dy[:4]}_{end_dy[:4]}.nc\")\n",
        "csv_path = os.path.join(data_dir, \"ERA5\", f\"era5_{start_dy[:4]}_{end_dy[:4]}.csv\")\n",
        "\n",
        "#Load NetCDF\n",
        "ds = xr.open_dataset(nc_file)\n",
        "tp = ds['tp']\n",
        "\n",
        "# Extract nearest point (xarray handles interpolation via method=\"nearest\")\n",
        "tp_point = tp.sel(latitude=lat, longitude=lon, method=\"nearest\")\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df_era5 = tp_point.to_dataframe().reset_index()[[\"datetime\", \"tp\"]]\n",
        "df_era5.columns = ['Date', 'Rainfall_mm']\n",
        "\n",
        "# Save CSV\n",
        "df_era5.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"CSV saved at: {csv_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU4c3mxonIm5"
      },
      "source": [
        "# **CHIRPS Daily: Climate Hazards Center InfraRed Precipitation With Station Data (v2.0 Final)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj8rrrFTz3W8"
      },
      "source": [
        "| Attribute              | Details                                                                 |\n",
        "|------------------------|-------------------------------------------------------------------------|\n",
        "| **Product Name**       | CHIRPS Daily: Climate Hazards Center InfraRed Precipitation With Station Data (v2.0 Final) |\n",
        "| **Horizontal Coverage**| Global (50° S to 50° N, all longitudes)                                 |\n",
        "| **Horizontal Resolution** | 0.05° × 0.05° (~5.566 km)                                            |\n",
        "| **Temporal Coverage**  | 1981-01-01 to Present                                               |\n",
        "| **Temporal Resolution**| Daily                                                                   |\n",
        "| **Units**              | mm/day                                                                  |\n",
        "| **Data Sources**       | Combination of satellite observations, rain gauge data, and climatology from UCSB Climate Hazards Group |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S060TyHh0YfM"
      },
      "source": [
        "**Downloading CHIRPS Daily Rainfall Data for a Specific Location Using Google Earth Engine**\n",
        "\n",
        "This script uses the Google Earth Engine (GEE) Python API to retrieve CHIRPS daily precipitation data for specified date range at the nearest grid point to a specified location (lat/lon). The data is resampled to 0.25° resolution, extracted for each day, stored in a pandas DataFrame, and saved as a CSV file for analysis.\n",
        "\n",
        "**Note:** To run this code, you need a Google Earth Engine account and an active project ID.\n",
        "\n",
        "**Procedure to get access:**\n",
        "\n",
        "\n",
        "\n",
        "1.   Create a Google Account (if you don’t have one).\n",
        "2.   Sign up for Google Earth Engine at https://signup.earthengine.google.com and wait for approval (usually 1–2 days).\n",
        "1.   Once approved, go to the Google Cloud Console → https://console.cloud.google.com.\n",
        "2.   Create a new project (or use an existing one) and note its Project ID.\n",
        "1.  Enable Earth Engine API for your project in the Cloud Console.\n",
        "2.  Install the earthengine-api Python package and authenticate using:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ss8Sz2fyPIIi"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "import pandas as pd\n",
        "import yaml\n",
        "import os\n",
        "\n",
        "# Load YAML config\n",
        "with open(\"config.yaml\", \"r\") as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "# Extract parameters\n",
        "data_dir = cfg[\"data_dir\"]\n",
        "output_dir = cfg[\"output_dir\"]\n",
        "lat = cfg[\"lat\"]\n",
        "lon = cfg[\"lon\"]\n",
        "start_date = cfg[\"start_date\"]\n",
        "end_date = cfg[\"end_date\"]\n",
        "gee_project_id = cfg.get(\"gee_project_id\")\n",
        "\n",
        "# Create CHIRPS output folder\n",
        "chirps_dir = os.path.join(data_dir, \"CHIRPS\")\n",
        "os.makedirs(chirps_dir, exist_ok=True)\n",
        "\n",
        "# --- Authenticate and Initialize Earth Engine ---\n",
        "if not gee_project_id:\n",
        "    raise ValueError(\"GEE Project ID is missing in config.yaml (gee_project_id).\")\n",
        "\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project=gee_project_id)\n",
        "\n",
        "# Point of interest\n",
        "point = ee.Geometry.Point([lon, lat])\n",
        "\n",
        "# Target resolution (in meters for ~0.25 degrees)\n",
        "target_scale_m = 27750\n",
        "\n",
        "# Load CHIRPS dataset\n",
        "collection = (\n",
        "    ee.ImageCollection(\"UCSB-CHG/CHIRPS/DAILY\")\n",
        "    .filterBounds(point)\n",
        "    .filterDate(start_date, end_date)\n",
        "    .select(\"precipitation\")\n",
        ")\n",
        "\n",
        "# Get list of images\n",
        "image_list = collection.toList(collection.size())\n",
        "n_images = image_list.size().getInfo()\n",
        "\n",
        "# Extract values\n",
        "dates, precip_values = [], []\n",
        "for i in range(n_images):\n",
        "    image = ee.Image(image_list.get(i))\n",
        "\n",
        "    # Resample to 0.25 degree resolution\n",
        "    resampled_image = (\n",
        "        image.reduceResolution(reducer=ee.Reducer.mean(), maxPixels=1024)\n",
        "        .reproject(crs=\"EPSG:4326\", scale=target_scale_m)\n",
        "    )\n",
        "\n",
        "    # Date\n",
        "    date = ee.Date(image.get(\"system:time_start\")).format(\"YYYY-MM-dd\").getInfo()\n",
        "\n",
        "    # Precipitation value at point\n",
        "    precip = resampled_image.reduceRegion(\n",
        "        reducer=ee.Reducer.first(), geometry=point, scale=target_scale_m\n",
        "    ).get(\"precipitation\")\n",
        "    precip_value = precip.getInfo() if precip else None\n",
        "\n",
        "    dates.append(date)\n",
        "    precip_values.append(precip_value)\n",
        "\n",
        "# Save DataFrame\n",
        "df_chirps = pd.DataFrame({\"date\": dates, \"precipitation (mm/day)\": precip_values})\n",
        "csv_path = os.path.join(chirps_dir, f\"chirps_{start_date[:4]}_{end_date[:4]}.csv\")\n",
        "df_chirps.to_csv(csv_path, index=False)\n",
        "\n",
        "print(df_chirps)\n",
        "print(f\"CSV saved: {csv_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtI8t830ggyY"
      },
      "source": [
        "# **GPM IMERG Final Precipitation L3 1 day 0.1 degree x 0.1 degree V07 (GPM_3IMERGDF) at GES DISC**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAHW2H1OqmBO"
      },
      "source": [
        "| Attribute           | Details                                                                 |\n",
        "|---------------------|-------------------------------------------------------------------------|\n",
        "| **Product Name**    | GPM IMERG Final Precipitation L3 Daily 0.1° × 0.1° V07 (GPM_3IMERGDF)   |\n",
        "| **Spatial Coverage**| Global (–90° to +90° latitude, –180° to +180° longitude)                |\n",
        "| **Spatial Resolution** | 0.1° × 0.1° (~10 km × 10 km)                                          |\n",
        "| **Temporal Coverage**| June 1, 2000 – ~February 28, 2025                                      |\n",
        "| **Temporal Resolution**| Daily (aggregated from 30-minute estimates)                          |\n",
        "| **Data Latency**    | ~4 months after month-end                                               |\n",
        "| **Primary Variable**| Precipitation (mm/day)                                                  |\n",
        "| **Other Variables** | precipitation_cnt, MWprecipitation_cnt, daily random error estimate    |\n",
        "| **Units**           | mm/day                                                                  |\n",
        "| **Data Source**     | NASA GES DISC (Final Run, gauge-adjusted, microwave + IR data)          |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owyDFITWozGL"
      },
      "source": [
        "**Downloading GPM IMERG Final Run Data Using Earthaccess**\n",
        "\n",
        "This script authenticates with NASA Earthdata via the earthaccess library, searches for GPM IMERG Final Run (Version 07) precipitation data for a specified time range and bounding box in specified region, and downloads the matching NetCDF files to a local directory.\n",
        "\n",
        "**Note:** An active Earthdata account is required to run this code and complete the login process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOFV2Wu779u2"
      },
      "source": [
        "[How to Access GES DISC Data Using Python](https://disc.gsfc.nasa.gov/information/howto?keywords=python&title=How%20to%20Access%20GES%20DISC%20Data%20Using%20Python)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDXt7AyDKZM4"
      },
      "outputs": [],
      "source": [
        "import netCDF4 as nc4\n",
        "import earthaccess\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "# --- Read YAML config ---\n",
        "with open(\"config.yaml\", \"r\") as f:  # adjust path if needed\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "data_dir = cfg[\"data_dir\"]\n",
        "lat = cfg[\"lat\"]\n",
        "lon = cfg[\"lon\"]\n",
        "\n",
        "# Bounding box offsets for ~0.7° N/S and ~0.55° E/W (adjust if needed)\n",
        "bbox = (lon - 0.55, lat - 0.7, lon + 0.55, lat + 0.7)\n",
        "\n",
        "start_dy = cfg.get(\"start_date\")\n",
        "end_dy = cfg.get(\"end_date\")\n",
        "\n",
        "# Output directory for IMERG\n",
        "imerg_dir = os.path.join(data_dir, \"IMERG\")\n",
        "os.makedirs(imerg_dir, exist_ok=True)\n",
        "\n",
        "# --- Authenticate with NASA Earthdata ---\n",
        "auth = earthaccess.login()\n",
        "\n",
        "print(\"Accessing Earth Data...\")\n",
        "# --- Search GPM IMERG Final Run data ---\n",
        "results = earthaccess.search_data(\n",
        "    short_name=\"GPM_3IMERGDF\",\n",
        "    version=\"07\",\n",
        "    temporal=(start_dy, end_dy),\n",
        "    bounding_box=bbox\n",
        ")\n",
        "print(\"Downloading data...\")\n",
        "# --- Download the data ---\n",
        "earthaccess.download(results, imerg_dir)\n",
        "\n",
        "print(f\"IMERG data downloaded to: {imerg_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1UNOSVlrco6"
      },
      "source": [
        "**Extracting IMERG Precipitation Data for a Specific Location and Saving as CSV**\n",
        "\n",
        "loading multiple IMERG .nc4 files to extract precipitation data for the nearest grid point to the specified station. The extracted points are converted into Pandas data frame and saved as csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbabO44_oxcd"
      },
      "outputs": [],
      "source": [
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "# --- Load YAML config ---\n",
        "with open(\"config.yaml\", \"r\") as f:  # Adjust path if config.yaml is elsewhere\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "data_dir = cfg[\"data_dir\"]\n",
        "lat = cfg[\"lat\"]\n",
        "lon = cfg[\"lon\"]\n",
        "station_name = cfg.get(\"station_name\", \"Unknown_Station\")\n",
        "start_dy = cfg[\"start_date\"]\n",
        "end_dy = cfg[\"end_date\"]\n",
        "\n",
        "# --- Path to IMERG folder ---\n",
        "imerg_dir = os.path.join(data_dir, \"IMERG\")\n",
        "\n",
        "# Get sorted list of .nc4 files\n",
        "nc_files = sorted(glob.glob(os.path.join(imerg_dir, \"*.nc4\")))\n",
        "if not nc_files:\n",
        "    raise FileNotFoundError(f\"No .nc4 files found in {imerg_dir}\")\n",
        "\n",
        "print(f\"Found {len(nc_files)} IMERG files.\")\n",
        "\n",
        "# Prepare list for results\n",
        "records = []\n",
        "\n",
        "# Process each file safely\n",
        "for i, file in enumerate(nc_files, start=1):\n",
        "    try:\n",
        "        ds = xr.open_dataset(file)\n",
        "\n",
        "        # Extract precipitation variable at nearest grid point\n",
        "        point_data = ds['precipitation'].sel(lat=lat, lon=lon, method='nearest')\n",
        "\n",
        "        # Convert to pandas DataFrame\n",
        "        df_temp = point_data.to_dataframe().reset_index()\n",
        "        df_temp = df_temp.rename(columns={'time': 'Date', 'precipitation': 'Rainfall_mm'})\n",
        "\n",
        "        records.append(df_temp)\n",
        "        ds.close()\n",
        "\n",
        "        if i % 100 == 0 or i == len(nc_files):\n",
        "            print(f\"Processed {i}/{len(nc_files)} files...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file}: {e}\")\n",
        "\n",
        "# Concatenate all results\n",
        "df_imerg = pd.concat(records, ignore_index=True)\n",
        "\n",
        "# Sort by date (optional, ensures correct order)\n",
        "df_imerg = df_imerg.sort_values(by=\"Date\").reset_index(drop=True)\n",
        "\n",
        "# Save to CSV\n",
        "output_csv_filename = os.path.join(imerg_dir, f\"imerg_{station_name}_{start_dy[:4]}_{end_dy[:4]}.csv\")\n",
        "df_imerg.to_csv(output_csv_filename, index=False)\n",
        "\n",
        "print(f\"IMERG data for {station_name} saved to: {output_csv_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVE5l1F3kktf"
      },
      "source": [
        "# **NWIC-IndiaWRIS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9Md3sh8-rRc"
      },
      "source": [
        "| Attribute                | Details                                                                 |\n",
        "|--------------------------|-------------------------------------------------------------------------|\n",
        "| **Data Source**          | NWIC – India Water Resources Information System (India-WRIS)            |\n",
        "| **Managed By**           | National Water Informatics Centre (NWIC), Ministry of Jal Shakti, Govt. of India|\n",
        "| **Rainfall Data Type**   | Rain gauge time-series (live), covering daily rainfall from 1901 to present|\n",
        "| **Spatial Coverage**     | Pan-India, across a dense network of central and state rain gauge stations|\n",
        "| **Temporal Coverage**    | 1901 – present (continuously updated) |\n",
        "| **Update Frequency**     | Real-time / live updates via WIMS platform|\n",
        "| **Access Platform**      | Data entered via WIMS (Water Information Management System) by central and state agencies, visualized and downloadable via India-WRIS portal|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoYam_BJAMPs"
      },
      "source": [
        "**Processing and Visualizing NWIC Rainfall Data from Raw SRG Measurements**\n",
        "\n",
        "This script processes rainfall data from NWIC-IndiaWRIS station CSVs by filtering Manual SRG (Standard Rain Gauge) readings, aggregating them into daily totals, and saving them as a clean CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRDyhKBF9WWq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import yaml\n",
        "import glob\n",
        "\n",
        "# --- Load YAML config ---\n",
        "config_path = r\"config.yaml\"  # full path to config.yaml\n",
        "with open(config_path, \"r\") as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "station_name = cfg.get(\"station_name\")\n",
        "start_dy = pd.to_datetime(cfg[\"start_date\"])\n",
        "end_dy = pd.to_datetime(cfg[\"end_date\"])\n",
        "\n",
        "# --- Paths ---\n",
        "base_dir = os.path.dirname(config_path)  # Folder where config.yaml is stored\n",
        "\n",
        "# Automatically find the Excel file in base_dir\n",
        "xlsx_files = glob.glob(os.path.join(base_dir, \"*.xlsx\"))\n",
        "if not xlsx_files:\n",
        "    raise FileNotFoundError(f\"No Excel file found in {base_dir}\")\n",
        "input_xlsx_path = xlsx_files[0]  # Take the first (and only) Excel file found\n",
        "print(f\"Reading Excel file: {input_xlsx_path}\")\n",
        "\n",
        "# Output directory from config.yaml\n",
        "output_dir = os.path.join(cfg[\"data_dir\"], \"NWIC\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "output_csv_path = os.path.join(output_dir, f\"nwic_{station_name}_{start_dy.year}_{end_dy.year}.csv\")\n",
        "plot_output_path = os.path.join(output_dir, f\"Rainfall_{station_name}_Plot.png\")\n",
        "\n",
        "# --- Read and process ---\n",
        "df_nwic = pd.read_excel(input_xlsx_path,sheet_name=1, skiprows=6)\n",
        "\n",
        "df_nwic.rename(columns={'Time': 'Data Time', 'Value': 'Data Value'}, inplace=True)\n",
        "df_nwic = df_nwic[df_nwic['Data Type Description'] == 'MANUAL-Rainfall - SRG(Standard Rain Gauge)']\n",
        "df_nwic['Data Time'] = pd.to_datetime(df_nwic['Data Time'])\n",
        "\n",
        "# Filter by date range\n",
        "df_nwic = df_nwic[(df_nwic['Data Time'] >= start_dy) & (df_nwic['Data Time'] <= end_dy)]\n",
        "df_nwic['Date'] = df_nwic['Data Time'].dt.date\n",
        "df_nwic = df_nwic.groupby('Date')['Data Value'].sum().reset_index()\n",
        "\n",
        "# --- Save as CSV ---\n",
        "df_nwic.to_csv(output_csv_path, index=False)\n",
        "\n",
        "# --- Plot ---\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df_nwic['Date'], df_nwic['Data Value'], marker='o', linestyle='-', color='blue')\n",
        "plt.axhline(y=50, color='red', linestyle='--', linewidth=2, label='Threshold = 50 mm')\n",
        "plt.title(f'{station_name} Daily Rainfall (Manual SRG Data)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Daily Rainfall (mm)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(plot_output_path)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Full NWIC rainfall CSV saved to: {output_csv_path}\")\n",
        "print(f\"Rainfall time series plot saved to: {plot_output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHDFajl8axg1"
      },
      "source": [
        "# **Accuracy Assessment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsV_3FdgBGuV"
      },
      "source": [
        "**Merging of Latest Rainfall Datasets (NWIC, IMD, ERA5, CHIRPS, IMERG) into a Single CSV**\n",
        "\n",
        "This script scans a specified directory for saved .csv files, loads each dataset into memory, applies a standardized preprocessing workflow, and merges all processed datasets into a single consolidated DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vt6DbL99hzL",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import yaml\n",
        "from glob import glob\n",
        "\n",
        "# === Load YAML config ===\n",
        "config_path = r\"config.yaml\"  # path to config.yaml\n",
        "with open(config_path, \"r\") as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "# === Build dataset directories from YAML ===\n",
        "data_base_dir = cfg[\"data_dir\"]\n",
        "data_dirs = {\n",
        "    \"NWIC\": os.path.join(data_base_dir, \"NWIC\"),\n",
        "    \"IMD\": os.path.join(data_base_dir, \"IMD\"),\n",
        "    \"ERA5\": os.path.join(data_base_dir, \"ERA5\"),\n",
        "    \"CHIRPS\": os.path.join(data_base_dir, \"CHIRPS\"),\n",
        "    \"IMERG\": os.path.join(data_base_dir, \"IMERG\"),\n",
        "}\n",
        "\n",
        "# === Get latest CSV file from each dataset folder ===\n",
        "files = {}\n",
        "for name, folder in data_dirs.items():\n",
        "    csv_files = glob(os.path.join(folder, \"*.csv\"))\n",
        "    if not csv_files:\n",
        "        raise FileNotFoundError(f\"No CSV files found in {folder}\")\n",
        "    latest_file = max(csv_files, key=os.path.getmtime)\n",
        "    files[name] = latest_file\n",
        "    print(f\"Using {name} file: {latest_file}\")\n",
        "\n",
        "# === Read and preprocess each dataset ===\n",
        "dfs = {}\n",
        "for name, path in files.items():\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.rename(columns={df.columns[0]: \"Date\"})\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"]).dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    if name == \"IMD\":\n",
        "        df = df.rename(columns={df.columns[1]: name})\n",
        "        df.iloc[:-1, 1] = df.iloc[1:, 1].values  # shift IMD values\n",
        "    elif name == \"IMERG\":\n",
        "        df = df.rename(columns={df.columns[3]: name})\n",
        "    else:\n",
        "        df = df.rename(columns={df.columns[1]: name})\n",
        "\n",
        "    dfs[name] = df\n",
        "\n",
        "# === Merge all datasets ===\n",
        "combined_df = dfs[\"NWIC\"]\n",
        "for name in [\"IMD\", \"ERA5\", \"CHIRPS\", \"IMERG\"]:\n",
        "    combined_df = combined_df.merge(dfs[name], on=\"Date\", how=\"outer\")\n",
        "\n",
        "# Sort by Date\n",
        "combined_df = combined_df.sort_values(\"Date\").reset_index(drop=True)\n",
        "\n",
        "# === Replace 0.01 with 0 and round ===\n",
        "for col in [\"NWIC\", \"IMD\", \"ERA5\", \"CHIRPS\", \"IMERG\"]:\n",
        "    if col in combined_df.columns:\n",
        "        combined_df[col] = combined_df[col].replace(0.01, 0).round(1)\n",
        "\n",
        "# === Drop lat/lon if they exist ===\n",
        "combined_df = combined_df.drop(columns=[c for c in [\"lat\", \"lon\"] if c in combined_df.columns])\n",
        "\n",
        "# === Save output ===\n",
        "output_path = os.path.join(data_base_dir, \"combined_rainfall_murappanaddu_latest.csv\")\n",
        "combined_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Combined DataFrame saved to: {output_path}\")\n",
        "display(combined_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_CwZ47NBGuV"
      },
      "outputs": [],
      "source": [
        "print(combined_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaKyw4XfBGuY"
      },
      "source": [
        "**Daily Rainfall Comparison with NSE Metrics**\n",
        "\n",
        "This script plots daily rainfall from multiple datasets against NWIC observations for the specified location, annotating each with its Nash–Sutcliffe Efficiency (NSE) to assess performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fxgH3BedzBz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Ensure Date column is datetime\n",
        "combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
        "\n",
        "# NSE function\n",
        "nse = lambda obs, sim: 1 - np.sum((obs - sim)**2) / np.sum((obs - obs.mean())**2)\n",
        "\n",
        "# Plot with Date on x-axis\n",
        "plt.figure(figsize=(15, 7))\n",
        "for col in ['IMD', 'ERA5', 'CHIRPS', 'IMERG']:\n",
        "    plt.plot(combined_df['Date'], combined_df[col],\n",
        "             label=f\"{col} (NSE: {nse(combined_df['NWIC'], combined_df[col]):.2f})\")\n",
        "\n",
        "plt.plot(combined_df['Date'], combined_df['NWIC'], label='NWIC')\n",
        "\n",
        "plt.title('Daily Rainfall Comparison in Murappanaddu')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Rainfall (mm)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3oNpr3LBGuY"
      },
      "source": [
        "**Monthly Rainfall Totals Comparison**\n",
        "\n",
        "This script aggregates daily rainfall data from multiple datasets into monthly totals and visualizes them for comparison at specified Location.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ag8zpwpn8Pz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure Date column is datetime\n",
        "combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n",
        "\n",
        "# Set Date as index for resampling\n",
        "combined_df = combined_df.set_index('Date')\n",
        "\n",
        "# Resample daily data to monthly sum (rainfall totals per month)\n",
        "monthly_combined_df = combined_df.resample('ME').sum()  # Use 'ME' instead of 'M' to avoid warning\n",
        "\n",
        "# Plot all columns\n",
        "plt.figure(figsize=(12, 6))\n",
        "for column in monthly_combined_df.columns:\n",
        "    plt.plot(monthly_combined_df.index, monthly_combined_df[column],\n",
        "             marker='o', linestyle='-', label=column)\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Monthly Total Rainfall (mm)')\n",
        "plt.title('Monthly Rainfall Comparison of Datasets (Murappanaddu)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZA4iLmGoUHK"
      },
      "source": [
        "# **Compute a panel of metrics (NSE, RMSE, bias, r)**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cjzxz1fnoSgY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Metric function\n",
        "def metrics(obs, sim):\n",
        "    mask = ~np.isnan(obs) & ~np.isnan(sim)\n",
        "    obs, sim = obs[mask], sim[mask]\n",
        "    if len(obs) < 2:\n",
        "        return [np.nan]*5\n",
        "    rmse = np.sqrt(mean_squared_error(obs, sim))\n",
        "    bias = np.mean(sim - obs)\n",
        "    corr = np.corrcoef(obs, sim)[0, 1]\n",
        "    r2 = r2_score(obs, sim)\n",
        "    nse = 1 - np.sum((obs - sim)**2) / np.sum((obs - np.mean(obs))**2)\n",
        "    return [nse, rmse, bias, corr, r2]\n",
        "\n",
        "# Calculate for all datasets in one go\n",
        "datasets = ['IMD', 'ERA5', 'CHIRPS', 'IMERG']\n",
        "metrics_df = pd.DataFrame(\n",
        "    [[d, *metrics(combined_df['NWIC'].values, combined_df[d].values)] for d in datasets],\n",
        "    columns=['Dataset', 'NSE', 'RMSE', 'Bias', 'Correlation', 'R2 Score']\n",
        ")\n",
        "\n",
        "display(metrics_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YFJo0rXJA4i"
      },
      "source": [
        "# **Contingency table (hits/misses)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuUqMpkNEbzm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "g, s = combined_df['NWIC'], combined_df['IMD']  # Gauge & Satellite\n",
        "thr = 100\n",
        "ge, se = g >= thr, s >= thr\n",
        "\n",
        "hits        = (ge & se).sum()\n",
        "misses      = (ge & ~se).sum()\n",
        "false_alarms= (~ge & se).sum()\n",
        "\n",
        "POD = hits / (hits + misses) if hits + misses else np.nan\n",
        "FAR = false_alarms / (hits + false_alarms) if hits + false_alarms else np.nan\n",
        "CSI = hits / (hits + misses + false_alarms) if hits + misses + false_alarms else np.nan\n",
        "\n",
        "print(f\"Threshold: {thr} mm\\nHits: {hits}, Misses: {misses}, False alarms: {false_alarms}\")\n",
        "print(f\"POD: {POD:.2f}, FAR: {FAR:.2f}, CSI: {CSI:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR-YNfmTBGuZ"
      },
      "source": [
        "# **Critical Sucess Index (CSI)**\n",
        "\n",
        "measures how well predicted rainfall events match observed events, considering both misses and false alarms.\n",
        "\n",
        "A higher CSI (closer to 1) means more accurate event detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrgJq_NQeWHc"
      },
      "outputs": [],
      "source": [
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "\n",
        "thr = [0.1, 1, 5, 10, 20, 50, 100]\n",
        "datasets = ['IMD', 'ERA5', 'CHIRPS', 'IMERG']\n",
        "\n",
        "def cont_metrics(obs, sim, t):\n",
        "    g, s = obs >= t, sim >= t\n",
        "    h, m, fa = (g & s).sum(), (g & ~s).sum(), (~g & s).sum()\n",
        "    return h/(h+m) if h+m else np.nan, fa/(h+fa) if h+fa else np.nan, h/(h+m+fa) if h+m+fa else np.nan\n",
        "\n",
        "metrics = [{'Threshold':t,'Dataset':d,**dict(zip(['POD','FAR','CSI'],cont_metrics(combined_df['NWIC'],combined_df[d],t)))}\n",
        "           for t in thr for d in datasets]\n",
        "df = pd.DataFrame(metrics)\n",
        "\n",
        "fig, ax = plt.subplots(3,1,figsize=(12,15),sharex=True)\n",
        "w, r = 0.15, np.arange(len(thr))\n",
        "for i,d in enumerate(datasets):\n",
        "    sub = df[df.Dataset==d]\n",
        "    for j,m in enumerate(['POD','FAR','CSI']):\n",
        "        ax[j].bar(r+i*w, sub[m], w, label=d if j==0 else \"\")\n",
        "        ax[j].set_ylabel(m)\n",
        "        ax[j].grid(True, axis='y', linestyle='--', alpha=0.7)  # Grid lines for all\n",
        "ax[0].set_title('Probability of Detection (POD)')\n",
        "ax[1].set_title('False Alarm Ratio (FAR)')\n",
        "ax[2].set_title('Critical Success Index (CSI)')\n",
        "ax[2].set_xticks(r+w*(len(datasets)-1)/2)\n",
        "ax[2].set_xticklabels(thr)\n",
        "ax[2].set_xlabel('Rainfall Threshold (mm)')\n",
        "ax[0].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Heatmap of CSI Rank by Rainfall Threshold**\n",
        "\n",
        "This heatmap shows the ranking of four rainfall datasets (IMD, ERA5, CHIRPS, IMERG) based on their Critical Success Index (CSI) across different rainfall thresholds. Lower rank numbers (1 = best) indicate better agreement between observed (NWIC) and dataset-predicted rainfall events.\n"
      ],
      "metadata": {
        "id": "db3YP8jtC0Qi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr1GE8Bem2tS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Thresholds and datasets\n",
        "thr = [0.1, 1, 5, 10, 20, 50, 100]\n",
        "datasets = ['IMD', 'ERA5', 'CHIRPS', 'IMERG']\n",
        "\n",
        "# Function to compute POD, FAR, CSI\n",
        "def cont_metrics(obs, sim, t):\n",
        "    g, s = obs >= t, sim >= t\n",
        "    h, m, fa = (g & s).sum(), (g & ~s).sum(), (~g & s).sum()\n",
        "    return h/(h+m) if h+m else np.nan, fa/(h+fa) if h+fa else np.nan, h/(h+m+fa) if h+m+fa else np.nan\n",
        "\n",
        "# Compute metrics\n",
        "metrics = [\n",
        "    {'Threshold': t, 'Dataset': d,\n",
        "     **dict(zip(['POD', 'FAR', 'CSI'], cont_metrics(combined_df['NWIC'], combined_df[d], t)))}\n",
        "    for t in thr for d in datasets\n",
        "]\n",
        "df_metrics = pd.DataFrame(metrics)\n",
        "\n",
        "# Pivot table for CSI\n",
        "csi_pivot = df_metrics.pivot(index=\"Dataset\", columns=\"Threshold\", values=\"CSI\")\n",
        "\n",
        "# Rank CSI values (1 = best, higher CSI gets lower rank number)\n",
        "csi_rank = csi_pivot.rank(ascending=False, axis=0).astype(int)\n",
        "\n",
        "# Heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(csi_rank, annot=True, fmt=\"d\", cmap=\"YlGn\",\n",
        "            cbar_kws={'label': 'Rank'}, linewidths=0.5, linecolor='gray')\n",
        "\n",
        "plt.title(\"Dataset CSI Rank by Threshold (1 = Best)\", fontsize=16, pad=12)\n",
        "plt.xlabel(\"Rainfall Threshold (mm)\")\n",
        "plt.ylabel(\"Dataset\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvwnkkgjI1gZ"
      },
      "source": [
        "# **Top-K event Comparision**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKo55uY8IRTt"
      },
      "outputs": [],
      "source": [
        "k=10\n",
        "top_gauge = combined_df.nlargest(k, 'NWIC')\n",
        "print(\"Top gauge events vs satellite values\")\n",
        "print(top_gauge.reset_index()[['Date','NWIC','IMERG']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D97x6kEVvGiI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "thr = 20\n",
        "g, s = top_gauge['NWIC'] >= thr, top_gauge['IMERG'] >= thr\n",
        "h, m, f, c = (g & s).sum(), (g & ~s).sum(), (~g & s).sum(), (~g & ~s).sum()\n",
        "\n",
        "# Contingency table\n",
        "table = pd.DataFrame(\n",
        "    [[h, m],\n",
        "     [f, c]],\n",
        "    index=['Sat Event: Yes', 'Sat Event: No'],\n",
        "    columns=['Gauge Event: Yes', 'Gauge Event: No']\n",
        ")\n",
        "\n",
        "# Metrics\n",
        "POD, FAR, CSI = h/(h+m), f/(h+f), h/(h+m+f)\n",
        "metrics = pd.DataFrame({\n",
        "    'Metric': ['POD (Probability of Detection)', 'FAR (False Alarm Ratio)', 'CSI (Critical Success Index)'],\n",
        "    'Value': [POD, FAR, CSI]\n",
        "})\n",
        "\n",
        "print(f\"Contingency Table for Top {k} Gauge Events (Threshold: {thr} mm)\\n\")\n",
        "display(table)\n",
        "\n",
        "print(\"\\nPerformance Metrics for Top Events:\\n\")\n",
        "display(metrics.style.format({'Value': '{:.2f}'}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qENGzc3RQ-AN"
      },
      "source": [
        "# **Scatter Plots**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyDLk8y_PHPp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "import numpy as np\n",
        "\n",
        "datasets = ['IMD', 'ERA5', 'CHIRPS', 'IMERG']\n",
        "gauge_col = 'NWIC'\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "all_vals = np.concatenate([combined_df[gauge_col].values] + [combined_df[ds].values for ds in datasets])\n",
        "gmin, gmax = np.nanmin(all_vals), np.nanmax(all_vals)\n",
        "\n",
        "for ax, dataset in zip(axes, datasets):\n",
        "    sns.scatterplot(x=combined_df[gauge_col], y=combined_df[dataset], s=40, alpha=0.5, ax=ax)\n",
        "    ax.plot([gmin, gmax], [gmin, gmax], 'r--', linewidth=1.5, label='1:1 line')\n",
        "\n",
        "    # Filter out NaNs for regression\n",
        "    temp_df = combined_df[[gauge_col, dataset]].dropna()\n",
        "    if not temp_df.empty:\n",
        "      X = temp_df[gauge_col].values.reshape(-1, 1)\n",
        "      y = temp_df[dataset].values\n",
        "      y_pred = LinearRegression().fit(X, y).predict(X)\n",
        "      ax.plot(temp_df[gauge_col], y_pred, color='blue', linewidth=2, label='Regression line')\n",
        "\n",
        "      r2 = r2_score(temp_df[gauge_col], y)\n",
        "      bias = np.mean(y - temp_df[gauge_col])\n",
        "\n",
        "      ax.text(0.05, 0.95, f\"R² = {r2:.2f}\\nBias = {bias:.2f}\", transform=ax.transAxes,\n",
        "              fontsize=10, verticalalignment='top', bbox=dict(facecolor='white', alpha=0.7, edgecolor='gray'))\n",
        "\n",
        "\n",
        "    ax.set_title(f\"{dataset} vs Gauge\", fontsize=14)\n",
        "    ax.set_xlabel(\"Gauge Rainfall (mm)\", fontsize=12)\n",
        "    ax.set_ylabel(f\"{dataset} Rainfall (mm)\", fontsize=12)\n",
        "    ax.set_xlim(gmin, gmax)\n",
        "    ax.set_ylim(gmin, gmax)\n",
        "    ax.legend()\n",
        "\n",
        "fig.suptitle(\"Satellite vs Gauge Rainfall Comparison (Murappanaddu, 2022–2024)\",\n",
        "             fontsize=16, fontweight='bold')\n",
        "fig.text(0.5, 0.92, \"Closer to red dashed line = better agreement\",\n",
        "         ha='center', fontsize=12, style='italic')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rp8aC55e_5pC",
        "GtI8t830ggyY"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}